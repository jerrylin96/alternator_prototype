{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define encoder class ###\n",
    "\n",
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dims):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_shape, int(input_shape/2))\n",
    "        self.linear2 = nn.Linear(int(input_shape/2), int(input_shape/3))\n",
    "        self.linear3 = nn.Linear(int(input_shape/3), int(input_shape/4))\n",
    "        self.linear4 = nn.Linear(int(input_shape/4), latent_dims) #mu\n",
    "        self.linear5 = nn.Linear(int(input_shape/4), latent_dims) #logstd\n",
    "        \n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        #self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        #self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        mu =  self.linear4(x)\n",
    "        sigma = torch.exp(self.linear5(x))\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).mean()\n",
    "        return z\n",
    "    \n",
    "### Define conditional decoder class ###\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_shape, target_shape, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dims+target_shape, int(input_shape/4))\n",
    "        self.linear2 = nn.Linear(int(input_shape/4)+target_shape, int(input_shape/3))\n",
    "        self.linear3 = nn.Linear(int(input_shape/3)+target_shape, int(input_shape/2))\n",
    "        self.linear4 = nn.Linear(int(input_shape/2)+target_shape, input_shape)\n",
    "        \n",
    "    def forward(self, z, targets):\n",
    "        ## lines for vanilla VAE (no conditioning)\n",
    "        #z = F.relu(self.linear1(z))\n",
    "        #z = torch.sigmoid(self.linear2(z))\n",
    "                                 \n",
    "        ## targets get concatenated to each layer output in decoder ##\n",
    "        z = F.relu(self.linear1(torch.cat((z, targets), 1))) \n",
    "        z = F.relu(self.linear2(torch.cat((z, targets), 1)))\n",
    "        z = F.relu(self.linear3(torch.cat((z, targets), 1)))\n",
    "        z = torch.sigmoid(self.linear4(torch.cat((z, targets), 1)))\n",
    "        \n",
    "        return torch.cat((z, targets), 1)\n",
    "    \n",
    "### CVAE class ###\n",
    "class CondVariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_shape, target_shape, latent_dims):\n",
    "        super(CondVariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(input_shape, latent_dims)\n",
    "        self.decoder = Decoder(input_shape, target_shape, latent_dims)\n",
    "    \n",
    "    def forward(self, x, targets):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z, targets)\n",
    "    \n",
    "### Function for KL annealing ###\n",
    "def anneal_schedule(epoch):\n",
    "    return min(epoch*0.1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function ###\n",
    "\n",
    "def train(autoencoder, data, epochs):\n",
    "    \n",
    "    recon_list = []\n",
    "    kl_list = []\n",
    "    \n",
    "    opt = torch.optim.Adam(autoencoder.parameters())\n",
    "    epoch_counter = 0\n",
    "    for epoch in epochs:\n",
    "        for i, batch in enumerate(tqdm(data, desc=\"Epoch Progress\")):\n",
    "            x = batch\n",
    "            x = x.to(device)\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            inp = x[:,:input_shape]\n",
    "            tar = x[:,-target_shape:]\n",
    "        \n",
    "            x_hat = autoencoder(inp, tar) #Reconstructed samples (with targets)\n",
    "            \n",
    "            mse = ((inp - x_hat[:,:len(inp[0])])**2).mean() #Reconstruction loss between input and recon data\n",
    "            kl = anneal_schedule(epoch_counter)*autoencoder.encoder.kl\n",
    "            loss = mse + kl\n",
    "            \n",
    "            var = torch.var(inp) \n",
    "            rsq = 1-mse/var #R^2 Evaluation metric\n",
    "            \n",
    "            recon_list.append(mse)\n",
    "            kl_list.append(kl)\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        epoch_counter += 1\n",
    "        if epoch_counter % 50 == 0:\n",
    "            print(\"Epoch %s \" % (epoch_counter), \"Loss: \", float(loss), \"R^2: \", float(rsq))\n",
    "        \n",
    "            PATH = \"/Users/dylansmith/Desktop/CS274E/Project/saved_models/cvae_epoch%s.pt\" \\\n",
    "            % (epoch_counter)\n",
    "            torch.save({\n",
    "                'epoch': epoch_counter,\n",
    "                'model_state_dict': autoencoder.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'recon_loss': recon_list,\n",
    "                'kl_loss': kl_list\n",
    "                }, PATH)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data (using validation data for debugging) ###\n",
    "\n",
    "path_input = '/Users/dylansmith/Desktop/CS274E/Project/val_input.npy'\n",
    "data_input = torch.Tensor(np.load(path_input))\n",
    "\n",
    "path_target = '/Users/dylansmith/Desktop/CS274E/Project/val_target.npy'\n",
    "data_target = torch.Tensor(np.load(path_target))\n",
    "\n",
    "data = torch.cat((data_input, data_target), 1)\n",
    "\n",
    "train_dataloader = DataLoader(data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 6\n",
    "input_shape = len(data_input[0])\n",
    "target_shape = len(data_target[0])\n",
    "print(\"Number of data features: \", input_shape)\n",
    "print(\"Number of data targets: \", target_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)\n",
    "\n",
    "cvae = CondVariationalAutoencoder(input_shape, target_shape, latent_dims).to(device) # GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training loop ###\n",
    "\n",
    "epoch_tot = 400\n",
    "epochs = tqdm(range(1, epoch_tot + 1), desc=\"Epochs\")\n",
    "\n",
    "vae = train(cvae, train_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load model for further evaluation/testing ###\n",
    "\n",
    "opt = torch.optim.Adam(cvae.parameters())\n",
    "\n",
    "PATH = \"/Users/dylansmith/Desktop/CS274E/Project/saved_models/cvae_epoch400.pt\"\n",
    "checkpoint = torch.load(PATH, map_location=device)\n",
    "cvae.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "curr_epoch = checkpoint['epoch']\n",
    "mse_loss = checkpoint['recon_loss']\n",
    "kl_loss = checkpoint['kl_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
